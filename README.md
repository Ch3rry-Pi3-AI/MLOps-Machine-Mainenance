# ğŸš€ **Training Pipeline â€” MLOps Machine Maintenance**

This branch advances the **MLOps Machine Maintenance** project by introducing the **`training_pipeline.py`** module inside the `pipeline/` directory.
It represents the **third executable workflow stage** of the project â€” combining **data preprocessing** and **model training** into a single, fully automated pipeline.

The training pipeline enables **end-to-end execution** of the machine learning workflow: from raw sensor data ingestion to model evaluation and persistence â€” all within one streamlined script.

## ğŸ§© **Overview**

The `training_pipeline.py` file orchestrates the projectâ€™s two key stages:

1ï¸âƒ£ **Data Processing** â€” loads raw data, performs cleaning, encoding, scaling, and saves train/test splits.
2ï¸âƒ£ **Model Training** â€” loads processed data, trains a Logistic Regression model, evaluates it, and saves the trained model to disk.

Both stages are powered by the core modules in `src/`:

* `data_processing.py`
* `model_training.py`
* `logger.py`
* `custom_exception.py`

This structure ensures that the workflow remains **reproducible**, **traceable**, and ready for **CI/CD integration**.

## ğŸ”§ **Core Responsibilities**

| Stage | Operation              | Description                                                                                                          |
| ----: | ---------------------- | -------------------------------------------------------------------------------------------------------------------- |
|   1ï¸âƒ£ | **Data Preprocessing** | Loads `data.csv`, cleans data, encodes categorical columns, standardises features, and saves processed artefacts.    |
|   2ï¸âƒ£ | **Model Training**     | Loads processed data, trains a Logistic Regression model, saves it as `model.pkl`, and logs key performance metrics. |

## ğŸ—‚ï¸ **Updated Project Structure**

```text
mlops_machine_maintenance/
â”œâ”€â”€ .venv/                           # ğŸ§© Local virtual environment (created by uv)
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ data.csv                 # âš™ï¸ Raw machine sensor dataset
â”‚   â”œâ”€â”€ processed/                   # ğŸ’¾ Processed data artefacts (train/test splits, scaler)
â”‚   â”‚   â”œâ”€â”€ X_train.pkl
â”‚   â”‚   â”œâ”€â”€ X_test.pkl
â”‚   â”‚   â”œâ”€â”€ y_train.pkl
â”‚   â”‚   â”œâ”€â”€ y_test.pkl
â”‚   â”‚   â””â”€â”€ scaler.pkl
â”‚   â””â”€â”€ models/                      # ğŸ§  Trained model artefacts
â”‚       â””â”€â”€ model.pkl
â”œâ”€â”€ pipeline/                        # âš™ï¸ Workflow orchestration layer
â”‚   â””â”€â”€ training_pipeline.py          # ğŸš€ End-to-end pipeline (data processing â†’ model training)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ custom_exception.py          # Unified and detailed exception handling
â”‚   â”œâ”€â”€ logger.py                    # Centralised logging configuration
â”‚   â”œâ”€â”€ data_processing.py           # ğŸ§© Data preprocessing, scaling & splitting
â”‚   â””â”€â”€ model_training.py            # âš™ï¸ Model training, evaluation, and persistence
â”œâ”€â”€ static/                          # ğŸ“Š Visual or diagnostic assets
â”œâ”€â”€ templates/                       # ğŸ§© Placeholder for web/API templates
â”œâ”€â”€ .gitignore                       # ğŸš« Git ignore rules
â”œâ”€â”€ .python-version                  # ğŸ Python version pin
â”œâ”€â”€ pyproject.toml                   # âš™ï¸ Project metadata and uv configuration
â”œâ”€â”€ requirements.txt                 # ğŸ“¦ Python dependencies
â”œâ”€â”€ setup.py                         # ğŸ”§ Editable install support
â””â”€â”€ uv.lock                          # ğŸ”’ Locked dependency versions
```

## âš™ï¸ **How to Run the Training Pipeline**

After ensuring your raw dataset is available at `artifacts/raw/data.csv`, run the entire workflow with a single command:

```bash
python pipeline/training_pipeline.py
```

### âœ… **Expected Successful Output**

```console
2025-11-08 14:30:51,105 - INFO - Data processing initialised.
2025-11-08 14:30:51,432 - INFO - Basic data preprocessing completed.
2025-11-08 14:30:51,879 - INFO - Train/test splits and scaler saved successfully.
2025-11-08 14:30:52,210 - INFO - Model training initialised.
2025-11-08 14:30:52,622 - INFO - Model trained and saved successfully.
2025-11-08 14:30:53,002 - INFO - Accuracy : 0.85 ; Precision : 0.84 ; Recall : 0.85 ; F1 : 0.84
2025-11-08 14:30:53,145 - INFO - End-to-end training pipeline executed successfully.
```

This confirms that:

* The preprocessing and model training stages were executed sequentially.
* Artefacts were successfully written to `artifacts/processed/` and `artifacts/models/`.
* Evaluation metrics were logged for performance tracking.

## ğŸ§  **Implementation Highlights**

* **End-to-End Automation**
  Runs both preprocessing and model training in one script, simplifying experimentation and integration with CI/CD tools.

* **Integrated Logging** via `src/logger.py`
  Captures timestamped logs for every major step, creating a full audit trail for debugging and reproducibility.

* **Unified Error Handling** via `src/custom_exception.py`
  Standardises error messages and traceback details for clear, contextual debugging.

* **Production-Ready Architecture**
  The pipeline structure mirrors real-world MLOps patterns â€” modular, version-controlled, and scalable for future extensions.

## ğŸ§© **Integration Guidelines**

| File                            | Purpose                                                           |
| ------------------------------- | ----------------------------------------------------------------- |
| `pipeline/training_pipeline.py` | Orchestrates the full ML workflow from preprocessing to training. |
| `src/data_processing.py`        | Handles data cleaning, encoding, scaling, and persistence.        |
| `src/model_training.py`         | Performs model training, saving, and evaluation.                  |
| `src/logger.py`                 | Centralises logging across the pipeline.                          |
| `src/custom_exception.py`       | Provides structured, traceable error handling.                    |

## âœ… **In Summary**

This stage transforms the **MLOps Machine Maintenance** project into a **complete, end-to-end machine learning workflow**.
With a single command, the `training_pipeline.py` script orchestrates data preprocessing, model training, and evaluation â€” producing reproducible artefacts and detailed logs.

It lays the groundwork for **CI/CD automation**, **Kubeflow pipeline integration**, and **scalable model retraining workflows** in future stages of the project.
